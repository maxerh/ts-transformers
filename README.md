# Transformer Architectures for 1D data

Small comparison of attention mechanisms.

## Attention mechanisms
Includes:
- Standard Attention
- Swin Attention
- Chunked Attention
- Linear Attention
- Sparse Attention
- LogSparse Attention
- Focal Attention

## Usage

``main.py`` is using the attention mechanisms and executes a performance test (time and memory consumption) for defined input window lengths. 
The results are saved in a csv-file. 
